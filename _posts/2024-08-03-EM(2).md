---
title : EM (2)
date : 2024-08-03 13:15:00 +09:00
categories : [EM]
tag : [EM]
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# The EM algorithm

machine learning과 통계학에서 ML 이나 MAP 파라미터 estimate는 관련된 무작위 변수들의 값들을 관측하여 쉽게 생산된다.
하지만, missing data 또는 latent variables를 갖는다면, ML/MAP estimate를 찾는것은 어렵다.


보편적인 경사도 기반의 optimizer는 **negative log likelihood(NLL)**의 local minumum을
찾는 것이다.   
$$NLL(\theta) = - \trangleq \frac{1}{N} \log p(D | \theta) $$


하지만 때때로 covariance matrices는 positive하게정의되어야 한다는 등의 제한사항을 강제해야 한다. 

그러한 경우 **expectation maximization(EM)**이라 불리는 알고리즘을 사용할 수 있다.
+ simple iterative 알고리즘이다.
+ 종종 각 step 마다 closed-form updates를 갖는다.
+ 이 알고리즘은 요구되는 제한사항을자동적으로 강제한다


EM은 만약 데이터가 모두 관측되었으면, ML/MAP estimate는 계산하기 쉽다는 사실을 이용한다.   
+ 주어진 파라미터에서 missing value를 추론하는 단계(E step)
+ 주어진 "채워진" data에서 파라미터들을 최적화하는 단계(M step)
+ 위 단계를 통해 대체하는 반복적인 알고리즘이다.


## Basic idea

$$x_i$$가 case $$i$$에서 관측 가능하거나 관측된 변수들이라 할때, $$z_i$$는 hidden또는
missing 변수들이다. 목표는 observed data의 log likelihood를 최대화 하는 것이다.

$$l(\theta) = \sum_{i=1}^{N} \log p(x_i | \theta) = \sum_{i=1}^{N} \log[\sum_{z_i} p(x_i, z_i | \theta)] $$


위 식은 최적화 하기 어렵다. log를 sum 내부로 넣을 수 없기 때문이다.

EM은 이 문제를 다음과 같이 우회한다. **complete data log likelihood**를 정의한다:   
$$l_c(\theta) \triangleq \sum_{i=1}^{N} \log p(x_i, z_i | \theta). $$


이는 계산될 수 없다. $$z_i$$를 모르기 때문이다. 
따라서 **expected complete data log likelihood**를 다음과 같이 정의한다:   
$$Q(\theta, \theta^{t-1}) = \mathbb{E}[l_c(\theta) | D, \theta^{t-1} ] $$.  
+ $$t$$는 current iteration number이다.
+ $$Q$$는 **auxiliary function**이라 불린다.
+ expectation은 old 파라미터 $$\theta^{t-1}$$와 observed data $$D$$에 대해서 취해진다.
+ **E step**의 목표는 MLE가 의존하는 내부항목 보다는
$$Q(\theta, \theta^{t-1})$$를 계산하는 것이다.
+ 이는 **expected sufficient statistics(ESS)**로 알려져 있다.

+ **M step**에서, $$\theta$$에 대한 Q function을 최적화 한다:   
$$\theta^t = \text{argmax}_{\theta} Q(\theta, \theta^{t-1})$$
    + MAP estimation을 수행하기 위해, M step을 다음과 같이 수정한다
    + $$\theta^t = \text{argmax}_{\theta} Q(\theta, \theta^{t-1} + \log p(\theta))$$
 
+ E stp은 변하지 않은 채로 둔다


## EM for GMMs
이 장에서는 EM을 이용해 mixture of Gaussians를 어떻게 최적화하는지에 대해 알아본다.   
다른 종류의 mixture models를 수정하기 위해서는 간단한 수정이 필요하다.   
mixture components의 수는 $$K$$라고 가정한다.

**Auxiliary function**   
expected complete data log likelihood는 다음과 같다:   
$$Q(\theta, \theta^{(t-1)}) \triangleq \text{E}[\sum_i \log p(x_i, z_i | \theta)] $$   
$$ = \sum_i \mathbb{E}[\log[ \pod_{k=1}^{K} (\pi_k p (x_i | \theta_k))^{\parallel (z_i = k)}]] $$   
$$ = \sum_i \sum_k \mathbb{E}[\parallel (z_i = k)] \log[\pi_k p(x_i | theta_k)]$$   
$$ = \sum_i \sum_k p(z_i = k | x_i, \theta^{t-1}) \log[\pi_k p(x_i | \theta_k)] $$   
$$ = \sum_i \sum_k r_{ik} \log \pi_k + \sum_i \sum_k r_{ik} \log p(x_i | \theta_k) $$   

+ 이때, $$r_{ik} \triangleq p(z_i = k | x_i, \theta^{(t-1)} $$은 cluster $$k$$가 
각 data point $$i$$에 갖는 **responsibility**이다.


**E step**   
E step은 다음의 간단한 형식을 갖는다. 이는 어느 mixture model과 같다:   
$$r_{ik} = \frac{\pi_k p(x_i | \theta_{k}^{t-1})}{\sum_{k'} \pi_{k'} p(x_i | \theta_{k'}^{(t-1)})}$$   


**M step**   
M step에서, $$\pi$$와 $$\theta_k$$에 대하여 $$Q$$를 최적화한다. $$\pi$$에 대해서 다음과 같다:   
$$\pi_k = \frac{1}{N} \sum_i r_{ik} = \frac{r_k}{N} $$   
이때 $$r_k \triangleq \sum_i r_{ik}$$는 군집 $$k$$에 할당된 points의 가중된 수이다.   

$$\mu_k$$와 $$\Sigma_k$$ 항일때 M step을 유도하기 위해서, 
$$\mu_k$$와 $$\Sigma_k$$에 의존하는 $$Q$$의 부분을 봐야 한다. 결과는 다음과 같다.   
$$l(\mu_k, \Sigma_k) = \sum_{k} \sum_{i} r_{ik} \log p(x_i | \theta_k)$$   
$$ = - \frac{1}{2} \sum_{i} r_{ik} [\log \lvert \Sigma_k \rvert + (x_i - \mu_k)^T \Sigma_{k}^{-1} (x_i - \mu_k)] $$   
이는 MVN의 MLEs를 계산하는 standard problem의 weighted 버전이다.   
새로운 parameter estimates는 다음과 같다:   
$$\mu_k = \frac{\sum_i r_{ik}x_i}{r_k}$$   
$$\Sigma_k = \frac{\sum_{i} r_{ik} x_i x_{i}^{T}}{r_k} - \mu_k \mu_{k}^{T}$$   
+ 이러한 식들은 다음과 같은 직관을 제시한다:
    + 군집 $$k$$의 평균은 군집 $$k$$에 할당된 모든 points들의 weighted average이다.
    + 공분산은 weighted empirical scatter matrix에 비율적이다.

새로운 estimates를 계산한 뒤에, 
$$k = 1: K$$일때 $$\theta^t = (\pi_k, \mu_k, \Sigma_k)$$로 설정하고 다음 E step으로 넘어간다.   


**Example**   
+ 조건: $$\mu_1 = (-1, 1), \Sigma_1 = I, \mu_2 = (1, -1), \Sigma_2 = I$$   
+ 군집 1에서 온 points를 파란색으로, 2에서 온 points를 빨간색으로 색칠한다:
$$\text{color}(i) = r_{i1}\text{blue} + r_{i2}\text{red}$$
    + 애매한 points는 보라색을 띈다.
20회 후에, 알고리즘은 좋은 군집화에 수렴한다.


**K-means algorithm**   
GMMs를 위한 다양한 EM 알고리즘이 있다. 그 예시가 K-means algorithm이다.   
GMMs에는 다음과 같은 가정이 필요하다: $$\Sigma_k = \sum_k = \Sigma^2 I_D$$가 고정되고, 
$$\pi_k = 1/K$$도 고정되었을때, 군집의 중심들, $$\mu_k \in R^D$$, 만을 estimate하면 된다.   
+ E step동안 계산되는 사후확률에 대한 delta-function approximation:
    + $$p(z_i = k | x_i, \theta) \approx \parallel (k = z_{i}^{*}) $$
    + 이때, $$z_{i}^{*} = \text{argmax}_{k} p(z_{i}=k \| x_{i}, \theta)$$

위는 군집들에 대한 points의 hard assignment 때문에 **hard EM**이라고도 불린다.
+ 각 군집에 대해 equal spherical covariance matrix를 가정했지 때문에,
대부분의 probable cluster for $$x_i$$는 nearest prototype을 찾음으로써 계산된다:
    + $$z_{i}^{*} = \text{argmin}_k \| x_i - \mu_k \|_{2}^{2}$$

따라서 각 E step마다, $$N$$ datapoints와 $$K$$ 군집 중심들 사이의 유클리드 거리를 찾아야 한다. 
이는 약 $$O(NKD)$$의 시간이 걸린다.   
주어진 hard cluster assignments에서, M step은 각 cluster center를 
그것에 할당된 모든 points의평균을 계산함으로써 업데이트한다:   
$$\mu_k = \frac{1}{N_k} \sum_{i:z_i = k} x_i$$


### Algorithm 11.1: K-means algorithm
1. *initialize* $$m_k$$
2. repeat
3.     가장 가까운 군집의 중심에 각 data point를 할당한다: 
$$z_i = \text{argmin}_k  \| x_i - \mu_k \|_{2}^{2}$$;   
4. 각군집을 그것에 할당된 모든 points의 평균을 계산함으로써 업데이트한다:   
$$mu_k = \frac{1}{N} \sum_{i:z_{i}=k} x_i$$;   
5. until *converged*;


**Vector quantization**   
K-means가 적절한 EM알고리즘이 아니기 때문에 likelihood를최대화 할 수 없다. 
이는 데이터 압축과 관련된 loss function을 근사적으로 최소화하는 greedy 알고리즘으로 해석된다.   

lossy compression of some real-valued vectors를 수행하길 원한다고 가정한다.
+ 기본 아이디어: K prototypes $$\mu_k \in \mathbb{R}^D$$의
 **codebook** 내부의 인덱스인 discrete symbol $$z_i \in {1, \dots, K}$$를
갖는 $$x_i \in \mathbb{R}$$인 각 real-valued vector를 교체하는 것이다.

+ 각 데이터 벡터는 가장 비슷한 prototypes의 인덱스를 이용하여 encoded된다.
+ 유사도는 유클리드 거리로 계산된다:
$$ \text{encode}(x_i) = \text{argmin}_{k} \| x_i - \mu_k |\^2$$

+ codebook의 질을 평가하는 cost function을 **reconstruction error**
또는 **distortion**을 계산함으로써 정의할 수 있다:

$$J(\mu, z |K,X) \triangleq \frac{1}{N} \sum_{i=1}^{N} \| x_i - \text{decode(encode(}x_i)) \|^2 = \frac{1}{N} \sum_{i=1}^{N} \|x_i -\mu_{zi} \|^2$$
+ 이때 decode($$k$$) = $$\mu_k$$다

K-means 알고리즘은 이 objective를 최소화하기 위한 간단한 반복 scheme이라 생각될 수 있다.



