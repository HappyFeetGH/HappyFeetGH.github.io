---
title : EM (2)
date : 2024-08-03 13:15:00 +09:00
categories : [EM]
tag : [EM]
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# The EM algorithm

machine learning과 통계학에서 ML 이나 MAP 파라미터 estimate는 관련된 무작위 변수들의 값들을 관측하여 쉽게 생산된다.
하지만, missing data 또는 latent variables를 갖는다면, ML/MAP estimate를 찾는것은 어렵다.


보편적인 경사도 기반의 optimizer는 **negative log likelihood(NLL)**의 local minumum을
찾는 것이다.   
$$NLL(\theta) = - \trangleq \frac{1}{N} \log p(D | \theta) $$


하지만 때때로 covariance matrices는 positive하게정의되어야 한다는 등의 제한사항을 강제해야 한다. 

그러한 경우 **expectation maximization(EM)**이라 불리는 알고리즘을 사용할 수 있다.
+ simple iterative 알고리즘이다.
+ 종종 각 step 마다 closed-form updates를 갖는다.
+ 이 알고리즘은 요구되는 제한사항을자동적으로 강제한다


EM은 만약 데이터가 모두 관측되었으면, ML/MAP estimate는 계산하기 쉽다는 사실을 이용한다.   
+ 주어진 파라미터에서 missing value를 추론하는 단계(E step)
+ 주어진 "채워진" data에서 파라미터들을 최적화하는 단계(M step)
+ 위 단계를 통해 대체하는 반복적인 알고리즘이다.


## Basic idea

$$x_i$$가 case $$i$$에서 관측 가능하거나 관측된 변수들이라 할때, $$z_i$$는 hidden또는
missing 변수들이다. 목표는 observed data의 log likelihood를 최대화 하는 것이다.

$$l(\theta) = \sum_{i=1}^{N} \log p(x_i | \theta) = \sum_{i=1}^{N} \log[\sum_{z_i} p(x_i, z_i | \theta)] $$


위 식은 최적화 하기 어렵다. log를 sum 내부로 넣을 수 없기 때문이다.

EM은 이 문제를 다음과 같이 우회한다. **complete data log likelihood**를 정의한다:   
$$l_c(\theta) \triangleq \sum_{i=1}^{N} \log p(x_i, z_i | \theta). $$


이는 계산될 수 없다. $$z_i$$를 모르기 때문이다. 
따라서 **expected complete data log likelihood**를 다음과 같이 정의한다:   
$$Q(\theta, \theta^{t-1}) = E[l_c(\theta) | D, \theta^{t-1} ] $$.  
+ $$t$$는 current iteration number이다.
+ $$Q$$는 **auxiliary function**이라 불린다.
+ expectation은 old 파라미터 $$\theta^{t-1}$$와 observed data $$D$$에 대해서 취해진다.
+ **E step**의 목표는 MLE가 의존하는 내부항목 보다는
$$Q(\theta, \theta^{t-1})$$를 계산하는 것이다.
+ 이는 **expected sufficient statistics(ESS)**로 알려져 있다.

+ **M step**에서, $$\theta$$에 대한 Q function을 최적화 한다:   
$$\theta^t = \text{argmax}_{\theta} Q(\theta, \theta^{t-1})$$
    + MAP estimation을 수행하기 위해, M step을 다음과 같이 수정한다
    + $$\theta^t = \text{argmax}_{\theta} Q(\theta, \theta^{t-1} + \log p(\theta))$$
 
+ E stp은 변하지 않은 채로 둔다


## EM for GMMs
이 장에서는 EM을 이용해 mixture of Gaussians를 어떻게 최적화하는지에 대해 알아본다.   
다른 종류의 mixture models를 수정하기 위해서는 간단한 수정이 필요하다.   
mixture components의 수는 $$K$$라고 가정한다.

**Auxiliary function**   
expected complete data log likelihood는 다음과 같다:   
$$Q(\theta, \theta^{(t-1)} \triangleq \text{E}[\sum_i \log p(x_i, z_i | \theta)] $$   
$$ = \sum_i E[\log[ \pod_{k=1}^{K} (\pi_k p (x_i | \theta_k))^{\parallel (z_i = k)}]] $$   
$$ = \sum_i \sum_k E[\parallel (z_i = k)] \log[\pi_k p(x_i | theta_k)]$$   
$$ = \sum_i \sum_k p(z_i = k | x_i, \theta^{t-1}) \log[\pi_k p(x_i | \theta_k)] $$   
$$ = \sum_i \sum_k r_{ik} \log \pi_k + \sum_i \sum_k r_{ik} \log p(x_i | \theta_k) $$   

+ 이때, $$r_{ik} \triangleq p(z_i = k | x_i, \theta^{(t-1)} $$은 cluster $$k$$가 
각 data point $$i$$에 갖는 **responsibility**이다.


**E step**   
E step은 다음의 간단한 형식을 갖는다. 이는 어느 mixture model과 같다:   
$$r_{ik} = \frac{\pi_k p(x_i | \theta_{k}^{t-1})}{\sum_{k'} \pi_{k'} p(x_i | \theta_{k'}^{(t-1)})}$$   


**M step**   
M step에서, $$\pi$$와 $$\theta_k$$에 대하여 $$Q$$를 최적화한다. $$\pi$$에 대해서 다음과 같다:   
$$\pi_k = \frac{1}{N} \sum_i r_{ik} = \frac{r_k}{N} $$   
이때 $$r_k \triangleq \sum_i r_{ik}$$는 군집 $$k$$에 할당된 points의 가중된 수이다.




