---
title : EM (1)
date : 2024-07-29 11:15:00 +09:00
categories : [EM]
tag : [EM]
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Parameter estimation for mixutre modles

파라미터들은 안다고 가정하고 주어진 observed variables에 대한 hidden variables에 대한 사후확률을 계산하는 방법을 보는 것을 알아왔다.
이 장에선, 파라미터들을 학습하는 방법에 대해 토론한다.

10.4.2 장에서 complete data와 factored prior를 가질 때, 파라미터들에 대한 사후확률을 factorize하고 쉽게 계산을 만든다.

하지만, 만약 hidden variables 나 missing data를 가질때, 이는 더 이상 참이 아니다. 
왜냐하면 $$z_i$$가 관측되었을 때, d-separation에 의해 $$\theta_z \perp \theta_{x} | D$$를 확인할 수 있다. 
따라서 사후확률을 factorize할 수 있다.

LVM에서, $$z_i$$는 hidden이고, 파라미터들은 더 이상 독립적이지 않게 된다. 그래서 사후확률은 factorize되지 않고, 이를 계산하기 더 어렵게 만든다. 아래와 같이 이는 MAP와 ML estimates의 계산을 복잡하게 만든다.

## Unidentifiability

LVM에 대한 $$p(\theta|D)$$를 계산할 때의 주 문제:
 사후확률은 multiple modes(다수의 대푯값)을 갖는다.


이유: GMM을 고려할 때, $$z_i$$가 모두 관측되었다면, 파라미터들에 대한 unimodal posterior를 갖는다:


$$ p(\theta|D) = Dir(\pi |D)\prod_{k=1}^{K} NIW(\mu_{k}, \sum_{k}|D)$$

+ 결론: globally optimal MAP estimate를 쉽게 찾을 수 있다.


하지만 $$z_i$$들이 hidden일때, $$z_i$$로 패워진 가능한 방법들의 각각에 대해서 서로다른 unimodla likelihood를 얻는다.


따라서 $$z_i$$들에 대해서 marginalize out할때, 
$$p(\theta|D)$$를 위한 multi-modal posterior를 얻을 수 있다.

이러한 대푯값들은 군집들의 다른 labelings에 대응한다.

![image](https://github-production-user-asset-6210df.s3.amazonaws.com/141888688/353304106-dcfd1f67-6b64-43d9-b30f-f4c0eeda0dc0.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240730T034906Z&X-Amz-Expires=300&X-Amz-Signature=01f7ec8e94c8d43cfb77cb31ce93517ba5cdc4101e6b455e4de251cd4c36dc08&X-Amz-SignedHeaders=host&actor_id=141888688&key_id=0&repo_id=758327526)

위에 설명되어 있다시피, 데이터에대한 K=2이고 2D GMM인 
likelihood function $$p(D| \mu_1,\mu_2)$$을 그린다.

두 정점을 확인할 수 있다. 
+ $$\mu_1 = -10$$, $$\mu_2 =10$$ 
+ $$\mu_1 = 10$$, $$\mu_2 = -10$$

파라미터들은 **identifiable**하지 않다. 특별한 MLE가 없기 때문이다.
따라서 특별한 MAP estimate(사전확률이 특정 labelings를 지배하지 못한다고 가정)이 없다.
=> 사후확률은 반드시 multimodal이 된다.


파라미터 사후확률에 얼마나 많은 대푯값들이 있는지는 대답하기 어렵다. 
$$K!$$개의 가능한 labelings가 있다.그들 중 일부 peaks는 합쳐질 수 있다.

그럼에도 불구하고 지수적 숫자가 될 수 있다. GMM일때 최적의 MLE를 찾는 것은 NP-hard이기 때문이다.


식별 불가능(Unidentifiability)는 Bayesian 추론일때 문제를 유발한다
+ 예시: 사후확률로부터 샘플을 뽑는다 - $$\theta^{(s)} \sim p(\theta|D)$$,
이후 이들을 평균내어 사후확률 평균
$$\bar{\theta} = \frac{1}{S}\sum_{s=1}^{S}\theta^{s}$$의 근사치를 구하는 시도를 한다.

+ 만약 샘플이 다른 대푯값으로부터 온다면, 평균은 의미 없어진다

+ 주의) likelihood function이 파라미터들이 온 대푯값으로부터 불변하기 때문에
posterior predictive distribution을 평균내는 것은 유의미하다.

  
식별 불가능 문제를 해결하기 위한 다양한 해결법
해결법들은 모델의 세부 사항과 그들이 사용하는 추론 알고리즘에 의존한다.
+ 예시) MCMC를 이용한 mixture 모델에서 식별불가능을 다루기 위한 방법을 (Stephens 2000)가 시도했다.

이번 장에서 사용할 접근법: single local mode를 계산한다
+ 예시) MAP estimation 근사치 구하기를 수행한다.
    + 위 방법은 그 단순성 때문에 보통의 방법과는 거리가 있다.
    + 유의미한 근사치이다. 적어도 샘플 크기가 충분히 크다면
        + 이유: $$N$$개의 latent variables를 확인한다. 각각은 data point 각각을 볼 수 있다.
그러나, latent 파라미터들은 둘 뿐이고, 각각은 $$N$$ data point만을 볼 수 있다.
따라서 파라미터에 대한 사후 확률 불완정성은 전형적으로 latent variables에 대한 사후확률 불안정성보다 낮다.
        + 이는 $$p(z_{i} | x_{i},\hat{\theta})$$를 계산하는 기본적인 전략을 정당화한다.
하지만 $$p(\theta | D)를 계산하는 것을 방해하지 않는다.


## Computing a MAP estimate is non-convex

이전 장에서, 방법론적으로 likelihood function이 다수의 대푯값을 갖는다고 알아보았다.
따라서, MAP나 ML estimate를 찾는 것이 매우 어려울 것이다.
이 장에서는, 이 결과를 문제에 대한 추가적 관점을 저장하는 대수적의미로 증명한다.

$$\log{p(D|\theta)}=\sum_{i}\log{[\sum_{z_i} p(x_i, z_{i}|\theta)]}$$

위 목표는 최대화 하기 어렵다. 왜냐하면 sum 내부에 로그를 집어넣을 수 없기 때문이다.
이는 특정 대수적 간결성을 거부한다, 그러나 이 문제가어렵다는 것을 증명하지 못한다.


이제 joint probability distribution $$p(z_i, x_{i}|\theta)$$는 
다음과 같이 쓰일 수 있는 exponential family임을 의미한다:

$$ p(x,z|\theta) = \frac{1}{Z(\theta)} \exp[\theta^T \phi(x,z)]$$

$$ \phi(x,z)$$는 sufficient statistics이다. $$Z(\theta)$$는 
normalization constant이다. 


MVN은 지금까지 우리가 만나왔던 분포들과 비슷하게 exponential family이다.

더 나아가 mixtures of expoenetial families들은 또한 expoenetial family에 속해있다.
이때 관측된 mixing indicator varibales를 제공한다.

이 가정에서, **complete data log likelihood**는 다음과 같다:
$$l_c(\theta) = \sum_{i} \log p(x_i, z_{i} | \theta) = \theta^T (\sum_{i} \phi(x_i, z_i)) - NZ(\theta)$$


첫 번째 항은 $$\theta$$에 대해 분명하게 선형이다.
$$Z(\theta)$$가 convex function이기 때문에 전반적인 objective는 concave하다.
따라서 특정한 최댓값을 가진다.


missing data가 있을 때 생기는 일:
+ **oberseved data log likelihood**는 다음과 같다:
    + $$l(\theta) = \sum_{i} \log \sum_{z_i} p(x_i, z_i | \theta) = \sum_{i} \log [\sum{z_i} e^{\theta^T \phi(z_i,x_i)}] - N \log Z(\theta)$$

+ 확인 가능한 것들: 위의 log-sum-exp function은 convex하다. 
$$Z(\theta)$$는 convex 하다. 하지만 두 convex functions의 차이는 보통 convex하지 않다.
따라서 obejective는 convex 하지도 concave하지도 않고, local optima를 갖는다.


non-convex function의 단점:
+ global optimum를 찾기 어렵다.
대부분의 최적화 알고리즘은 그들이 시작하는 위치에 따라 local optimum를 찾는다.
+ global optimum를 찾으려는 시도가 있어 왔다. 하지만 비현실적인 가정 하에만 가능했다.
+ 실전에서 "좋은" local optimum을 찾기 위해 **multiple random restarts**를 이용해
local optimizer를 수행할 것이다.


mixutre of Gaussians를 fitting하기 위한 convex method가 제안되었다는 것을 주지해라.
+ 아이디어: data point마다 하나의 군집을 할당하고
convex $$l_1$$-type penalty를 이용하여 그들으로부터 하나를 선택한다.
+ 특성: sparse kernel logistic regression에서 사용되는 unsupervied 버전의 접근법이다.
+ 주의사항: $$l_1$$ penalty는 convex임에도 불구하고 sparsity를 향상시키기위한 좋은 방법은 아니다.
 


# The EM algorithm

machine learning과 통계학에서 ML 이나 MAP 파라미터 estimate는 관련된 무작위 변수들의 값들을 관측하여 쉽게 생산된다.
하지만, missing data 또는 latent variables를 갖는다면, ML/MAP estimate를 찾는것은 어렵다.


보편적인 경사도 기반의 optimizer는 **negative log likelihood(NLL)**의 local minumum을
찾는 것이다.   
$$NLL(\theta) = - \trangleq \frac{1}{N} \log p(D | \theta) $$


하지만 때때로 covariance matrices는 positive하게정의되어야 한다는 등의 제한사항을 강제해야 한다. 

그러한 경우 **expectation maximization(EM)**이라 불리는 알고리즘을 사용할 수 있다.
+ simple iterative 알고리즘이다.
+ 종종 각 step 마다 closed-form updates를 갖는다.
+ 이 알고리즘은 요구되는 제한사항을자동적으로 강제한다


EM은 만약 데이터가 모두 관측되었으면, ML/MAP estimate는 계산하기 쉽다는 사실을 이용한다.   
+ 주어진 파라미터에서 missing value를 추론하는 단계(E step)
+ 주어진 "채워진" data에서 파라미터들을 최적화하는 단계(M step)
+ 위 단계를 통해 대체하는 반복적인 알고리즘이다.


## Basic idea

$$x_i$$가 case $$i$$에서 관측 가능하거나 관측된 변수들이라 할때, $$z_i$$는 hidden또는
missing 변수들이다. 목표는 observed data의 log likelihood를 최대화 하는 것이다.

$$l(\theta) = \sum_{i=1}^{N} \log p(x_i | \theta) = \sum_{i=1}^{N} \log[\sum_{z_i} p(x_i, z_i | \theta)] $$


위 식은 최적화 하기 어렵다. log를 sum 내부로 넣을 수 없기 때문이다.

EM은 이 문제를 다음과 같이 우회한다. **complete data log likelihood**를 정의한다:   
$$l_c(\theta) \triangleq \sum_{i=1}^{N} \log p(x_i, z_i | \theta). $$


이는 계산될 수 없다. $$z_i$$를 모르기 때문이다. 
따라서 **expected complete data log likelihood**를 다음과 같이 정의한다:   
$$Q(\theta, \theta^{t-1}) = E[l_c(\theta) | D, \theta^{t-1} ] $$.  
+ $$t$$는 current iteration number이다.
+ $$Q$$는 **auxiliary function**이라 불린다.
+ expectation은 old 파라미터 $$\theta^{t-1}$$와 observed data $$D$$에 대해서 취해진다.
+ **E step**의 목표는 MLE가 의존하는 내부항목 보다는
$$Q(\theta, \theta^{t-1})$$르ㄹ 계산하는 것이다.
+ 이는 **expected sufficient statistics(ESS)**로 알려져 있다.







