---
title : EM (1)
date : 2024-07-29 11:15:00 +09:00
categories : [EM]
tag : [EM]
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Parameter estimation for mixutre modles

파라미터들은 안다고 가정하고 주어진 observed variables에 대한 hidden variables에 대한 사후확률을 계산하는 방법을 보는 것을 알아왔다.
이 장에선, 파라미터들을 학습하는 방법에 대해 토론한다.

10.4.2 장에서 complete data와 factored prior를 가질 때, 파라미터들에 대한 사후확률을 factorize하고 쉽게 계산을 만든다.

하지만, 만약 hidden variables 나 missing data를 가질때, 이는 더 이상 참이 아니다. 
왜냐하면 $$z_i$$가 관측되었을 때, d-separation에 의해 $$\theta_z \perp \theta_{x} | D$$를 확인할 수 있다. 
따라서 사후확률을 factorize할 수 있다.

LVM에서, $$z_i$$는 hidden이고, 파라미터들은 더 이상 독립적이지 않게 된다. 그래서 사후확률은 factorize되지 않고, 이를 계산하기 더 어렵게 만든다. 아래와 같이 이는 MAP와 ML estimates의 계산을 복잡하게 만든다.

## Unidentifiability

LVM에 대한 $$p(\theta|D)$$를 계산할 때의 주 문제:
 사후확률은 multiple modes(다수의 대푯값)을 갖는다.


이유: GMM을 고려할 때, $$z_i$$가 모두 관측되었다면, 파라미터들에 대한 unimodal posterior를 갖는다:


$$ p(\theta|D) = Dir(\pi |D)\prod_{k=1}^{K} NIW(\mu_{k}, \sum_{k}|D)$$

+ 결론: globally optimal MAP estimate를 쉽게 찾을 수 있다.


하지만 $$z_i$$들이 hidden일때, $$z_i$$로 패워진 가능한 방법들의 각각에 대해서 서로다른 unimodla likelihood를 얻는다.


따라서 $$z_i$$들에 대해서 marginalize out할때, 
$$p(\theta|D)$$를 위한 multi-modal posterior를 얻을 수 있다.

이러한 대푯값들은 군집들의 다른 labelings에 대응한다.

![image](https://github-production-user-asset-6210df.s3.amazonaws.com/141888688/353304106-dcfd1f67-6b64-43d9-b30f-f4c0eeda0dc0.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240730T034906Z&X-Amz-Expires=300&X-Amz-Signature=01f7ec8e94c8d43cfb77cb31ce93517ba5cdc4101e6b455e4de251cd4c36dc08&X-Amz-SignedHeaders=host&actor_id=141888688&key_id=0&repo_id=758327526)

위에 설명되어 있다시피, 데이터에대한 K=2이고 2D GMM인 
likelihood function $$p(D| \mu_1,\mu_2)$$을 그린다.

두 정점을 확인할 수 있다. 
+ $$\mu_1 = -10$$, $$\mu_2 =10$$ 
+ $$\mu_1 = 10$$, $$\mu_2 = -10$$

파라미터들은 **identifiable**하지 않다. 특별한 MLE가 없기 때문이다.
따라서 특별한 MAP estimate(사전확률이 특정 labelings를 지배하지 못한다고 가정)이 없다.
=> 사후확률은 반드시 multimodal이 된다.


파라미터 사후확률에 얼마나 많은 대푯값들이 있는지는 대답하기 어렵다. 
$$K!$$개의 가능한 labelings가 있다.그들 중 일부 peaks는 합쳐질 수 있다.

그럼에도 불구하고 지수적 숫자가 될 수 있다. GMM일때 최적의 MLE를 찾는 것은 NP-hard이기 때문이다.


식별 불가능(Unidentifiability)는 Bayesian 추론일때 문제를 유발한다
+ 예시: 사후확률로부터 샘플을 뽑는다 - $$\theta^{(s)} \sim p(\theta|D)$$,
이후 이들을 평균내어 사후확률 평균
$$\bar{\theta} = \frac{1}{S}\sum_{s=1}^{S}\theta^{s}$$의 근사치를 구하는 시도를 한다.

+ 만약 샘플이 다른 대푯값으로부터 온다면, 평균은 의미 없어진다

+ 주의) likelihood function이 파라미터들이 온 대푯값으로부터 불변하기 때문에
posterior predictive distribution을 평균내는 것은 유의미하다.

  
식별 불가능 문제를 해결하기 위한 다양한 해결법
해결법들은 모델의 세부 사항과 그들이 사용하는 추론 알고리즘에 의존한다.
+ 예시) MCMC를 이용한 mixture 모델에서 식별불가능을 다루기 위한 방법을 (Stephens 2000)가 시도했다.

이번 장에서 사용할 접근법: single local mode를 계산한다
+ 예시) MAP estimation 근사치 구하기를 수행한다.
    + 위 방법은 그 단순성 때문에 보통의 방법과는 거리가 있다.
    + 유의미한 근사치이다. 적어도 샘플 크기가 충분히 크다면
        + 이유: $$N$$개의 latent variables를 확인한다. 각각은 data point 각각을 볼 수 있다.
그러나, latent 파라미터들은 둘 뿐이고, 각각은 $$N$$ data point만을 볼 수 있다.
따라서 파라미터에 대한 사후 확률 불완정성은 전형적으로 latent variables에 대한 사후확률 불안정성보다 낮다.
        + 이는 $$p(z_{i} | x_{i},\hat{\theta})$$를 계산하는 기본적인 전략을 정당화한다.
하지만 $$p(\theta | D)를 계산하는 것을 방해하지 않는다.


## Computing a MAP estimate is non-convex

이전 장에서, 방법론적으로 likelihood function이 다수의 대푯값을 갖는다고 알아보았다.
따라서, MAP나 ML estimate를 찾는 것이 매우 어려울 것이다.
이 장에서는, 이 결과를 문제에 대한 추가적 관점을 저장하는 대수적의미로 증명한다.

$$\log{p(D|\theta)}=\sum_{i}\log{[\sum_{z_i} p(x_i, z_{i}|\theta)]}$$

위 목표는 최대화 하기 어렵다. 왜냐하면 sum 내부에 로그를 집어넣을 수 없기 때문이다.
이는 특정 대수적 간결성을 거부한다, 그러나이 문제가어렵다는 것을 증명하지 못한다.


이제 joint probability distribution $$p(z_i, x_{i}|\theta)$$는 
다음과 같이 쓰일 수 있는 exponential family임을 의미한다:

$$ p(x,z|\theta) = \frac{1}{Z(\theta)} \exp[\theta^T \phi(x,z)]$$

$$ \pi(x,z)$$는 sufficient statistics이다. $$Z(\theta)$$는 
normalization constant이다. 


MVN은 지금까지 우리가 만나왔던 분포들과 비슷하게 exponential family이다.

더 나아가 mixtures of expoenetial families들은 또한 expoenetial family에 속해있다.
이때 관측된 mixing indicator varibales를 제공한다.

이 가정에서, **complete data log likelihood**는 다음과 같다:
$$l_c(\theta) = \sum_{i} \log p(x_i, z_{i} | \theta) = \theta^T (\sum_{i} \phi(x_i, z_i)) - NZ(\theta)$$


첫 번째 항은 $$\theta$$에 대해 분명하게 선형이다.
$$Z(\theta)$$가 convex function이기 때문에 전반적인 objective는 concave하다.
따라서 특정한 최댓값을 가진다.


missing data가 있을 때 생기는 일: **oberseved data log likelihood**는 다음과 같다:
$$l(\theta) = \sum_{i} \log \sum_{z_i} p(x_i, z_i | \theta) = \sum_{i} \log [\sum{z_i} \e^{\theta^T \phi(z_i,x_i)}] - N \log Z(\theta)$$









