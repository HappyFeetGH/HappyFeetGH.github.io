---
title : HMM (1)
date : 2024-07-27 07:15:00 +09:00
categories : [MachineLearning, HMM]
tag : [hmm]
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# HMM 알아보기 (1)

파라미터들 $ \theta = (\pie, A, B) $ 를 어떻게 측정할 것인가에 대해 의논한다
+ $$\pie(i) = p(z_1 = i)$$는 initial state distribution 
+ $$A(i,j) = p(z_t =j | z_{t-1} = i)$$는 transition matrix 
+ $$B$$는 class-conditional densities $$p(x_t|z_t=j)$$의 파라미터들이다

+ 첫째로 $$z_{1:T}$$가 training set엣관측되었을 때 경우를 고려한다. 그리고나서 $$z_{1:T}$$가 hidden인경우의 harder case를 확인한다.


### Training with fully observed data
만약 hidden state sequences를 관측한다면 $$A$$나 $$\pie$$를 위한 MLEs를 정확하게 계산할 수 있다. 만약 conjugate prior를 사용한다면, 더 쉽게 사후확률을 계산할 수 있다.

+ observation model의 형태에 의존하는 $$B$$를 어떻게 평가하는지:
		+ 상황은 generative classifier를 fitting하는 것과 비슷하다
		+ 예시: 각 state가 그것과 관련된 parameters $$B_{jl} = p(X_t = l|z_t = j)$$를 갖고
				 $$l \in \{ 1, \dots, L \}$$ 인 multinoulli distribution을 가질 때,
					observed symbol을 다음과 같이 표현한다
				+ MLE: $$ \hat{B_{jl}} = \frac{N^X_jl}{N_{j}}, N^X_{jl} \triangleq \sum{i=1}{N}\sum{t=1}{T_i} \parallel (z_{i,t} = j, x_{i,t} = l ) $$
		+ 위 결과가 말하는 것: state $$j$$ 에있는 횟수를 추가하고 symbol $$l$$ 을 확인한다. 그 후 state $$j$$ 에 있는 횟수만큼 나누면 된다.

유사하게, 만약 state가 그것과 관계된 Gaussian distribution을 갖는다면,다음과 같은 MLEs를 갖는다:
$$ \hat{\mu_k} = \frac{\bar{x}_k}{N_k}, \hat{\sum{}{k}} = \frac{(\bar{x}\bar{x})^T_k - N_k \hat{\mu_k} \hat{\mu^T_k}}{N_k}$$

$$\bar{x}_k \triangleq \sum_{i=1}^{N} \sum_{t=1}^{T_i} \parallel (z_{i,t} = k)x_{i,t}$$
$$ (\bar{x}\bar{x})^T_k \triangleq \sum{i=1}{N} 
		\sum{t=1}{T_i} \parallel (z_{i,t} = k) x_{i,t} x^T_{i,t} $$

다른 종류의 분포에도 유사한 결과가 나온다. MAP 측정값 또는 파라미터에 대한 full 사후확률으ㄹ 구하기 위해 모든 이러한 결과들의 확장이 쉽게 가능하다

### EM for HMMs(the Baum-Welch algorithm)

만약 $$z_t$$ 변수가 관측되지 않았다면, mixture model을 최적화 하는 것과 유사한 상황이 된다.
가장 보편적인 접근법은 MLE나 MAP 파라미터들을 찾기 위해서 EM 알고리즘을 사용하는 것이다. gradient-based 바ㅇ법을 사용할 수도 있다.
이 장에서, EM 알고리즘을 유도한다. HMMs에 적용될 때, 이는 **Baum-Welch** 알고리즘으로 알려져있다.

1. E step
expected complete data log likelihood는 직관적으로 다음과 같다:


$$ Q(\theta, theta^{old}) = sum_{k=1}^{K} E[N_{k}^{l}] \log \pi_k + \sum_{j=1}^{K} \sum_{k=1}^{K} E[N_{jk}] \log A_{jk} + 
		\sum_{i=1}^N \sum_{t=1}^{T_i} \sum_{k=1}^{K} p(z_t = k|x_i , \theta^{old}) \log p(x_{i,t} |\phi_k) $$

이때 expected counts 는 다음과 같다


$$ E[N_{k}^{l}] =  \sum_{i=1}^{N} p(z_{il} = k | x_{i}, \theta^{old}) $$



$$ E[N_{jk}] = \sum_{i=1}^{N} \sum_{t=2}^{T_i} p(z_{i,t-1} = j, z_{i,t} = k | x_{i}, \theta^{old})$$



$$ E[N_j] = \sum_{i=1}^{N} \sum_{t=1}^{T_i} p(z_{i,t} = j | x_{i}, \theta^{old}) $$


이러한 expected sufficient statistics는 각 sequence에 대한 forwards-backwards를 수행함으로써 계산될 수 있다.
특히, 알고리즘은 다음의 smoothed node와 edge marginals를 계산한다


$$ \gamma_{i,t}(j) \triangleq p(z_t = j | x_{i,l:T_i}, \theta) $$

$$ \xi_{i,t}(j,k) \triangleq p(z_{t-1} = j, z_t = k | x_{i,1:T_i}, \theta) $$

2. M step

$$A$$와 $$\pi$$를 위한 M step은 expected counts를 normalize하는 것이다.


$$\hat{A}_{jk} = \frac{E[N_{jk}}{\sum_{k'}E[N_{jk'}}, \hat{\pie}_k = \frac{E[N_{k}^{l}}{N}$$


이 결과는 직관적이다: $$j$$에서 $$k$$로 변환된 expected number를 단순히 더하고, $$j$$에서 다른 것으로 변환된 expected number 수로 나눈다.


multinoulli observation model에서, expected sufficient statistics는다음과 같다:


$$ E[M_{jl}] = \sum_{i=1}^{N}\sum_{t=1}^{T_i} \gamma_{i,t}(j) \parallel(x_{i,t} = l = \sum_{i=1}^{N} \sum_{t:x_{i,t}=l} \gamma_{i, t}(j) $$


M step은 다음의 형식을 갖는다: $$ \hat{B_{jl}} = \frac{E[M_{jl}]}{E[N_k]}	$$
